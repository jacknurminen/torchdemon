{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"torchdemon Documentation : https://jacknurminen.github.io/torchdemon Source Code : https://github.com/jacknurminen/torchdemon PyPI : https://pypi.org/project/torchdemon/ Inference Server for RL Inference Server . Serve model on GPU to workers. Workers communicate with the inference server over multiprocessing Pipe connections. Dynamic Batching . Accumulate batches from workers for forward passes. Set maximum batch size or maximum wait time for releasing batch for inference. Installation pip install torchdemon Usage Define a model import torch class Model(torch.nn.Module): def __init__(self, input_size: int, output_size: int): super(Model, self).__init__() self.linear = torch.nn.Linear(input_size, output_size) def forward(self, x: torch.Tensor) -> torch.Tensor: return self.linear(x) model = Model(8, 4) Create an inference server for the model import torchdemon inference_server = torchdemon.InferenceServer( model, batch_size=8, max_wait_ns=1000000, device=torch.device(\"cuda:0\") ) Create an inference client per agent and run in parallel processes import multiprocessing processes = [] for _ in range(multiprocessing.cpu_count()): inference_client = inference_server.create_client() agent = Agent(inference_client) process = multiprocessing.Process(target=play, args=(agent,)) process.start() processes.append(process) Run server inference_server.run() for process in processes: process.join() Development Clone this repository Requirements: Poetry Python 3.7+ Create a virtual environment and install the dependencies poetry install Activate the virtual environment poetry shell Testing pytest Documentation The documentation is automatically generated from the content of the docs directory and from the docstrings of the public signatures of the source code. The documentation is updated and published as a Github project page automatically as part each release. Releasing Trigger the Draft release workflow (press Run workflow ). This will update the changelog & version and create a GitHub release which is in Draft state. Find the draft release from the GitHub releases and publish it. When a release is published, it'll trigger release workflow which creates PyPI release and deploys updated documentation. Pre-commit Pre-commit hooks run all the auto-formatters (e.g. black , isort ), linters (e.g. mypy , flake8 ), and other quality checks to make sure the changeset is in good shape before a commit/push happens. You can install the hooks with (runs for each commit): pre-commit install Or if you want them to run only for each push: pre-commit install -t pre-push Or if you want e.g. want to run all checks manually for all files: pre-commit run --all-files This project was generated using the wolt-python-package-cookiecutter template.","title":"Introduction"},{"location":"#torchdemon","text":"Documentation : https://jacknurminen.github.io/torchdemon Source Code : https://github.com/jacknurminen/torchdemon PyPI : https://pypi.org/project/torchdemon/","title":"torchdemon"},{"location":"#inference-server-for-rl","text":"Inference Server . Serve model on GPU to workers. Workers communicate with the inference server over multiprocessing Pipe connections. Dynamic Batching . Accumulate batches from workers for forward passes. Set maximum batch size or maximum wait time for releasing batch for inference.","title":"Inference Server for RL"},{"location":"#installation","text":"pip install torchdemon","title":"Installation"},{"location":"#usage","text":"Define a model import torch class Model(torch.nn.Module): def __init__(self, input_size: int, output_size: int): super(Model, self).__init__() self.linear = torch.nn.Linear(input_size, output_size) def forward(self, x: torch.Tensor) -> torch.Tensor: return self.linear(x) model = Model(8, 4) Create an inference server for the model import torchdemon inference_server = torchdemon.InferenceServer( model, batch_size=8, max_wait_ns=1000000, device=torch.device(\"cuda:0\") ) Create an inference client per agent and run in parallel processes import multiprocessing processes = [] for _ in range(multiprocessing.cpu_count()): inference_client = inference_server.create_client() agent = Agent(inference_client) process = multiprocessing.Process(target=play, args=(agent,)) process.start() processes.append(process) Run server inference_server.run() for process in processes: process.join()","title":"Usage"},{"location":"#development","text":"Clone this repository Requirements: Poetry Python 3.7+ Create a virtual environment and install the dependencies poetry install Activate the virtual environment poetry shell","title":"Development"},{"location":"#testing","text":"pytest","title":"Testing"},{"location":"#documentation","text":"The documentation is automatically generated from the content of the docs directory and from the docstrings of the public signatures of the source code. The documentation is updated and published as a Github project page automatically as part each release.","title":"Documentation"},{"location":"#releasing","text":"Trigger the Draft release workflow (press Run workflow ). This will update the changelog & version and create a GitHub release which is in Draft state. Find the draft release from the GitHub releases and publish it. When a release is published, it'll trigger release workflow which creates PyPI release and deploys updated documentation.","title":"Releasing"},{"location":"#pre-commit","text":"Pre-commit hooks run all the auto-formatters (e.g. black , isort ), linters (e.g. mypy , flake8 ), and other quality checks to make sure the changeset is in good shape before a commit/push happens. You can install the hooks with (runs for each commit): pre-commit install Or if you want them to run only for each push: pre-commit install -t pre-push Or if you want e.g. want to run all checks manually for all files: pre-commit run --all-files This project was generated using the wolt-python-package-cookiecutter template.","title":"Pre-commit"},{"location":"api_docs/","text":"API documentation models InferenceInputData dataclass InferenceInputData(args: List[ForwardRef('np.ndarray')], kwargs: Dict[str, ForwardRef('np.ndarray')]) Source code in torchdemon/models.py @dataclass class InferenceInputData : args : List [ \"np.ndarray\" ] kwargs : Dict [ str , \"np.ndarray\" ] InferencePayload dataclass InferencePayload(client_id: uuid.UUID, data: torchdemon.models.InferenceInputData) Source code in torchdemon/models.py @dataclass class InferencePayload : client_id : UUID data : InferenceInputData InferenceRequest dataclass InferenceRequest(client_id: uuid.UUID, data: Union[torchdemon.models.InferenceInputData, torchdemon.models.Signal]) Source code in torchdemon/models.py @dataclass class InferenceRequest : client_id : UUID data : Union [ InferenceInputData , Signal ] InferenceResult dataclass InferenceResult(client_id: uuid.UUID, data: List[ForwardRef('np.ndarray')]) Source code in torchdemon/models.py @dataclass class InferenceResult : client_id : UUID data : List [ \"np.ndarray\" ] Signal ( IntEnum ) An enumeration. Source code in torchdemon/models.py class Signal ( IntEnum ): CLOSE = 0","title":"API documentation"},{"location":"api_docs/#api-documentation","text":"","title":"API documentation"},{"location":"api_docs/#torchdemon.models","text":"","title":"models"},{"location":"api_docs/#torchdemon.models.InferenceInputData","text":"InferenceInputData(args: List[ForwardRef('np.ndarray')], kwargs: Dict[str, ForwardRef('np.ndarray')]) Source code in torchdemon/models.py @dataclass class InferenceInputData : args : List [ \"np.ndarray\" ] kwargs : Dict [ str , \"np.ndarray\" ]","title":"InferenceInputData"},{"location":"api_docs/#torchdemon.models.InferencePayload","text":"InferencePayload(client_id: uuid.UUID, data: torchdemon.models.InferenceInputData) Source code in torchdemon/models.py @dataclass class InferencePayload : client_id : UUID data : InferenceInputData","title":"InferencePayload"},{"location":"api_docs/#torchdemon.models.InferenceRequest","text":"InferenceRequest(client_id: uuid.UUID, data: Union[torchdemon.models.InferenceInputData, torchdemon.models.Signal]) Source code in torchdemon/models.py @dataclass class InferenceRequest : client_id : UUID data : Union [ InferenceInputData , Signal ]","title":"InferenceRequest"},{"location":"api_docs/#torchdemon.models.InferenceResult","text":"InferenceResult(client_id: uuid.UUID, data: List[ForwardRef('np.ndarray')]) Source code in torchdemon/models.py @dataclass class InferenceResult : client_id : UUID data : List [ \"np.ndarray\" ]","title":"InferenceResult"},{"location":"api_docs/#torchdemon.models.Signal","text":"An enumeration. Source code in torchdemon/models.py class Signal ( IntEnum ): CLOSE = 0","title":"Signal"},{"location":"changelog/","text":"Changelog All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning . Unreleased 0.0.3 - 2022-01-16 Changed Return InferenceResult to inference client from server Return result ndarrays from InferenceClient forward 0.0.2 - 2022-01-16 Added inference_server Usage example to README Changed Set InferenceServer and InferenceClient as only public classes for torchdemon package 0.0.1 - 2022-01-14 Added inference_scheduler inference_client inference_model inference_queue","title":"Changelog"},{"location":"changelog/#changelog","text":"All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning .","title":"Changelog"},{"location":"changelog/#unreleased","text":"","title":"Unreleased"},{"location":"changelog/#003-2022-01-16","text":"","title":"0.0.3 - 2022-01-16"},{"location":"changelog/#changed","text":"Return InferenceResult to inference client from server Return result ndarrays from InferenceClient forward","title":"Changed"},{"location":"changelog/#002-2022-01-16","text":"","title":"0.0.2 - 2022-01-16"},{"location":"changelog/#added","text":"inference_server Usage example to README","title":"Added"},{"location":"changelog/#changed_1","text":"Set InferenceServer and InferenceClient as only public classes for torchdemon package","title":"Changed"},{"location":"changelog/#001-2022-01-14","text":"","title":"0.0.1 - 2022-01-14"},{"location":"changelog/#added_1","text":"inference_scheduler inference_client inference_model inference_queue","title":"Added"}]}